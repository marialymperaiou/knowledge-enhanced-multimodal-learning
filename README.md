# Knowledge-enhanced multimodal learning
This is a repository that provides a list of papers on knowledge-enhanced multimodal learning inspired by [Awesome Vision-and-Language](https://github.com/sangminwoo/awesome-vision-and-language).



## Surveys
- A survey on knowledge-enhanced multimodal learning (2022): https://arxiv.org/abs/2211.12328

## Datasets 
* **KB-VQA**:  Ask me anything: Free-form visual question answering based on knowledge from external sources https://arxiv.org/abs/1511.06973
* **Factual VQA (FVQA)**: FVQA: Fact-Based Visual Question Answering https://arxiv.org/abs/1606.05433
* **Knowledge-aware VQA (KVQA)**: Kvqa: Knowledge-aware visual question answering https://ojs.aaai.org/index.php/AAAI/article/view/4915
* **Outside-knowledge VQA (OK-VQA)**: OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge https://arxiv.org/abs/1906.00067
* **Text-KVQA**: From strings to things: Knowledge-enabled vqa model that can read and reason https://openaccess.thecvf.com/content_ICCV_2019/papers/Singh_From_Strings_to_Things_Knowledge-Enabled_VQA_Model_That_Can_Read_ICCV_2019_paper.pdf
* **Visual7W+KB**: Cross-modal knowledge reasoning for knowledge-based visual question answering https://arxiv.org/abs/2009.00145
* **S3VQA**: Select, substitute, search: A new benchmark for knowledge-augmented visual question answering https://arxiv.org/abs/2103.05568
* **Zero-shot Fact VQA (ZS-F-VQA)**: Zero-shot visual question answering using knowledge graph https://arxiv.org/abs/2107.05348
* **High-order Visual Question Reasoning (HVQR)**: Explainable High-order Visual Question Reasoning: A New Benchmark and Knowledge-routed Network https://arxiv.org/abs/1909.10128

## Models

### Knowledge-enhanced VQA

### Knowledge-enhanced multimodal retrieval

### Knowledge-enhanced visual reasoning

### Knowledge-enhanced visual storytelling

### Knowledge-enhanced image generation from text

### Multi-task models
